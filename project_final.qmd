---
title: "Final Project"
author: "Elisa Colson & Adriel Mazère"
format: 
  html: default
  pdf: default
---

```{r}
#| message: false
here::i_am("project_sandp_tweets.Rproj")
library(here)
library(quantmod)
library(vroom)
library(dplyr)
library(ggplot2)
library(gt)
library(tidyr)
library(stringr)
library(stargazer)
library(TTR)
```

## Introduction

We are interested in the relationship between Donald J. Trump's daily media rampages and how it affects the most famous international stock index, the S&P500. \hfill \break Before delving into analysis, we however need to analyse precisely and separately, then jointly, multiple datasets. \hfill \break Thereafter, multiple regression analysis will be conducted, especially on sub-samples, focusing on particular periods or events.

## Loading the data for the tweets ...

We obtain this data from Kaggle. This dataset contains statistics on DJT's tweeting since 2009 to 2021 until his account was banned. **Data for Truth Social is pending**

```{r}
# Loading the data and creating two sub-sets: one for original tweets 
# and another one for retweets
total_tweets <- vroom(here("data","tweets.csv")) |>
  mutate(date=as.Date(date)) |>
  arrange(date)
tweets <- vroom(here("data","tweets.csv")) |>
  mutate(date=as.Date(date)) |>
  filter(isRetweet == FALSE) |>
  arrange(date)
retweets <- vroom(here("data","tweets.csv")) |>
  mutate(date=as.Date(date)) |>
  filter(isRetweet == TRUE) |>
  arrange(date)
```

### Statistics on tweets and truths

#### Statistics on tweets

This set contains information on the date of the tweets, if they're retweets, from which device they were sent, but more importantly the text they contain, which will allow us to go deep into the wording. The tweet density can help us visualize the frenzy:

```{r}
# Plotting the density of original tweets
ggplot(tweets,aes(x=as.Date(date))) +
  geom_density(bw="sj") +
  annotate("rect", xmin = as.Date("2017-01-20"), 
           xmax = as.Date("2020-01-20"),
           ymin = 0, ymax = Inf, alpha = 0.2, fill = "red") +
  annotate("text", x = as.Date("2018-07-01"), y = 5e-04,
           label = "Trump 45", color = "red") +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y")
```

We can extract some useful nominal statistics out of this dataset:

```{r}
# Creating the statistics for original tweets
statistics_tweets <- tweets |>
  count(date)

# Creating the table
stargazer(as.data.frame(statistics_tweets), type="text")
```

On 3500 days of data, POTUS wrote, between 2009 and 2021 before his account was banned, 16 tweets on average per day, with the quietest day at 1 tweet per day and the busiest at 160 tweets per day. On average, the DJT twitted everyday every hour and a half, and on its busiest day, every 9 minutes.

```{r}
# Setting up a count for all tweets, that separates the number of 
# original tweets and retweets
daily_counts <- total_tweets |>
  arrange(date) |>
  mutate(type = if_else(isRetweet, "Retweet", "Original")) |>
  count(date, type, name = "n")

# Plotting additivelt tweets and retweets
ggplot(daily_counts, aes(x = as.Date(date), y = n, colour = type)) +
  geom_col() +
  scale_colour_manual(values = c(Original = "black", Retweet = "skyblue")) +
  annotate("rect", xmin = as.Date("2017-01-20"),xmax = as.Date("2020-01-20"),
           ymin = 0, ymax = Inf, alpha = 0.2, fill = "red") +
  annotate("text", x = as.Date("2018-07-01"), y = 120,
           label = "Trump 45", color = "red") +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  theme(legend.position = "none")
```

As per the plot, DJT started retweeting en-masse starting the end of its first term. During its presidency, the tweeting storm started low and then went on increasingly. During the period 2013 to 2015, he was particularly active.

For a smoother comparison, we merge the datasets from Twitter and Truth Social.

#### Statistics on truths

#### Statistics on both

## Loading the data from the S&P500

We get this data from Yahoo! Finance. We take daily data from 1970, the year during which DJT first twitted, up until today.

```{r}
# Loading the data for S&P500
getSymbols("^GSPC", src = "yahoo", 
           from = "2009-01-01",
           to   = Sys.Date())
sp500 <- data.frame(date=index(GSPC),coredata(GSPC))
sp500 <- sp500 |>
  arrange(date)
```

We can plot the closing prices to observe general events:

```{r}
# Plotting the closing price between 2009 and 2025
ggplot(sp500, aes(x=date, y=GSPC.Close)) +
  geom_line() +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  theme(axis.text.x = element_text(angle = 50, hjust = 1)) +
  annotate("rect", xmin = as.Date("2017-01-20"),xmax = as.Date("2020-01-20"),
           ymin = 0, ymax = Inf, alpha = 0.2, fill = "red") +
  annotate("text", x = as.Date("2018-07-01"), y = 5000,
           label = "Trump 45", color = "red") +
  annotate("rect", xmin = as.Date("2025-01-20"),xmax = as.Date(Sys.Date()),
           ymin = 0, ymax = Inf, alpha = 0.2, fill = "red") +
  annotate("text", x = as.Date("2025-04-01"), y = 2000,
           label = "Trump 47", color = "red")
```

More generally, the dataset is subdivided into 6 categories: Open, High, Low, Close, Volume and Adjusted. The latter is adjusted for dividends and stock splits. The formula for the adjustment factor $AF_t$ is:

$$
AF_{t}=\prod_{k=t+1}^{T}S_k \times \left(1-\frac{D_k}{P_{k-1}} \right)
$$ Where $S_k$ is the split factor on day $k$, $D_k$ the dividend per share paid on day $k$, $P_{k-1}$ the closing price the day before, the paid dividend $D_k$ and $T$ the last available day. The formula for adjusted price is therefore:

$$
\text{AdjClose}_t=\text{Close}_t \times AF_t
$$

### Statistics on S&P500

#### General statistics on S&P500

We can get useful data out of each category:

```{r}
# Creating the statistics for every column with every sub-set, for each price
sp500_stats <- sp500 |>
  select(where(is.numeric)) |> 
  summarise(across(everything(),
                   list(
                     Min= ~sprintf("%.2f", min(.x)),
                     Max= ~sprintf("%.2f", max(.x)),
                     Mean= ~sprintf("%.2f", mean(.x)),
                     StandDev= ~sprintf("%.2f", sd(.x))
                   ))) |>
  pivot_longer(everything(),
               names_to = c("Variable", ".value"),
               names_sep = "_")

# Creating the table
stargazer(as.data.frame(sp500_stats),
          type = "text",
          summary = FALSE)
```

These results are consistent with the evolution of the market. Typically, in 2009, the opening prices of the market were around 679 index points, reaching 6529 in 2025, with on average 2753 points and a deviation of 1436 around the mean. The close results also imply somewhat stability and consistence. Moreover, identical results for GSPC.Adjusted and GSPC.Close indicate an adjustment factor of 1.

#### Returns' statistics on S&P500

What is, however, most interesting to evaluate, are the daily returns, and in particular what one calls financial volatility, *i.e*, returns' fluctuations. Returns represent the relative change in prices from one day to another. They're computed using the formula:

$$
r_t=\frac{P_t}{P_{t-1}}-1
$$ 

Where $P_t$ is the price in day $t$. Volatility, then, is simply the standard deviation of returns. Let the sample mean, *i.e.*, the daily average of returns over $N$ days be:

$$
\bar{r}=\frac{1}{N}\sum_{t=1}^{N}r_t
$$ 

And the daily volatility, where we use the formula for the **unbiased** sample variance:

$$
\sigma_{\text{daily}}=\sqrt{\frac{1}{N-1}\sum_{t=1}^{N} \left(r_t - \bar r \right)^2}
$$ 
We use for $P_t$ the adjusted closing price:

The volatility observed here is realized volatility: it can be mainly used to measure the markets' reaction to Trump's tweets: higher volatility when Trump tweets en-masse, or about certain subjects, implies higher uncertainty and reveals a sensibility of the financial markets' to the buisnessman-president's media actions.

```{r}
# Creating the statistics for the return
sp500_return <- sp500 |>
  mutate(r = (GSPC.Adjusted/lag(GSPC.Adjusted))-1,
         rlog = log(GSPC.Adjusted / lag(GSPC.Adjusted))
           )|>
  drop_na()

sp500_return_stats <- sp500_return |>
  summarise(across(c(r,rlog),
                   list(
                     Min= ~sprintf("%.2f", min(.x) * 100),
                     Max= ~sprintf("%.2f", max(.x) * 100),
                     Mean= ~sprintf("%.2f", mean(.x) * 100),
                     Volatility= ~sprintf("%.2f", sd(.x) * 100)
                   ))) |>
  pivot_longer(everything(),
               names_to = c("Variable", ".value"),
               names_sep = "_")

# Creating the table
stargazer(as.data.frame(sp500_return_stats),
          type = "text",
          summary = FALSE)
```

Log-return displays approximately identical statistics. The worst daily loss is around -12% and the best daily gain around +9.5%, while on average one could expect a positive gain of +0.05%, for a daily swing around +1.15%. \hfill \break 
Given a typical trading year amounts to 252 days, annualized expected return is around 12.6%, while volatility is around 18%. This values are coherent with the literature. (**Je le sais, mais besoin de source quand même**). Plotting for visualization:

```{r}
ggplot(sp500_return, aes(x=date, y=r*100)) +
  geom_line() +
  annotate("rect", xmin = as.Date("2017-01-20"),xmax = as.Date("2020-01-20"),
           ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "red") +
  theme(axis.text.x = element_text(angle = 50, hjust = 1)) +
  annotate("text", x = as.Date("2018-07-01"), y = -7.5,
           label = "Trump 45", color = "red") +
  annotate("rect", xmin = as.Date("2025-01-20"),xmax = as.Date(Sys.Date()),
           ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "red") +
  annotate("text", x = as.Date("2025-04-01"), y = -7.5,
           label = "Trump 47", color = "red") +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y")
```

To plot the volatility, we could use the simple approximation for daily volatility:

$$
\sigma_{\text{approx}}=|r|
$$ This yields:

```{r}
ggplot(sp500_return, aes(x=date, y=abs(r)*100)) +
  geom_line() +
  annotate("rect", xmin = as.Date("2017-01-20"),xmax = as.Date("2020-01-20"),
           ymin = 0, ymax = Inf, alpha = 0.2, fill = "red") +
  theme(axis.text.x = element_text(angle = 50, hjust = 1)) +
  annotate("text", x = as.Date("2018-07-01"), y = 10,
           label = "Trump 45", color = "red") +
  annotate("rect", xmin = as.Date("2025-01-20"),xmax = as.Date(Sys.Date()),
           ymin = 0, ymax = Inf, alpha = 0.2, fill = "red") +
  annotate("text", x = as.Date("2025-04-01"), y = 10,
           label = "Trump 47", color = "red") +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y")
```

Yang & Zhang (2020), however, offered a much less noisy derivation for daily volatility, which is commonly used in finance, and that will be used for further analysis, called the OHLC (Open-High-Low-Close) volatility, $\sigma^2_{\text{YZ}}$, constructed on sample standard deviation from the formula:

$$
\sigma^2_{\text{YZ}}=\sigma^2_o+k\sigma^2_c+(1-k)\sigma^2_{RS}
$$ 
Where:

- $k=\frac{0.34}{1.34+\frac{N+1}{N-1}}$ is the bias adjustment
- $\sigma_o^2=\frac{1}{N-1}\sum_{t=1}^{N} \left(\ln \left(\frac{O_t}{C_{t-1}}\right)-\bar{r^{o}} \right)^2$ is the overnight variance (measures volatility from the close of $t-1$ to the open of $t$)
- $\sigma_c^2=\frac{1}{N-1}\sum_{t=1}^{N} \left(\ln \left(\frac{C_t}{O_{t}}\right)-\bar{r^{c}} \right)^2$ is the open-to-close variance (measures volatility of trading activities during $t$)
- $\sigma^2_{RS}=\frac{1}{N}\sum_{t=1}^{N} \left[\ln \left(\frac{H_t}{C_t} \right) \cdot \ln\left(\frac{H_t}{O_t} \right) + \ln \left(\frac{L_t}{C_t} \right) \cdot \ln\left(\frac{L_t}{O_t} \right)  \right]$ is the Rogers-Satchell variance (measures intraday range volatility)

With $N$ the number of days, $O_t$ the opening price, $C_t$ the closing price, $H_t$ the high-price and $L_t$ the low-price at day $t$, $\bar{r^{o}}$ and $\bar{r^{c}}$ are sample means of, respectively, $r^{o}_{t}=\ln\left(\frac{O_t}{C_{t-1}}\right)$ and $r^{c}_{t}=\ln\left(\frac{C_t}{O_{t}}\right)$. The volatility is therefore:

$$
\sigma_{\text{YZ}}=\sqrt{\sigma^2_{\text{YZ}}}
$$
The OHLC volatility is computed on a 21-days basis, simply using the TTR package which has a built-in function:
```{r}
# Creating OHLC volatility vector
ohlc <- sp500[, c("GSPC.Open", "GSPC.High", "GSPC.Low", "GSPC.Close")]
sp500 <- sp500 |>
  mutate(
    vol_yz = as.numeric(volatility(ohlc, n = 21, calc = "yang.zhang"))) |>
  drop_na()

# Ploting OHLC volatility
ggplot(sp500, aes(x=date, y=vol_yz*100)) +
  geom_line() +
  annotate("rect", xmin = as.Date("2017-01-20"),xmax = as.Date("2020-01-20"),
           ymin = 0, ymax = Inf, alpha = 0.2, fill = "red") +
  theme(axis.text.x = element_text(angle = 50, hjust = 1)) +
  annotate("text", x = as.Date("2018-07-01"), y = 50,
           label = "Trump 45", color = "red") +
  annotate("rect", xmin = as.Date("2025-01-20"),xmax = as.Date(Sys.Date()),
           ymin = 0, ymax = Inf, alpha = 0.2, fill = "red") +
  annotate("text", x = as.Date("2025-04-01"), y = 50,
           label = "Trump 47", color = "red") +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y")
```

We can clearly observe similar, although smoother, spikes, because YZ captures essentially regime-related market mood. Another useful series to be analysed would be the implied variance.

### Loading Options data for S&P500 (VIX)

The implied volatility represents the expectations of the market. Higher volatility is associated with higher risk or uncertainty, and therefore higher option prices. \hfill \break
We will use the VIX index (out of Yahoo! Finance again) for IV data, typically. Collecting Out-of-Money (OTM) calls and puts around many strikes for maturities at 30-days, let $Q(K_i)$ be the option mid-quote at strike $K_i$, the forward price is:

$$
F = K_0 + e^{rT}\,(C(K_0) - P(K_0))
$$
Where $C(\cdot)$ and $P(\cdot)$ are, respectively, call and put prices at the first strike and $T$ the time until expiry. The expected variance yields:

$$
\sigma^2_{\text{VIX}} = \frac{2}{T}\sum_i \frac{\Delta K_i}{K_i^2} e^{rT} Q(K_i)
- \frac{1}{T}\Big(\frac{F}{K_0} - 1\Big)^2
$$
Where $\Delta K_i$ represents the jump between adjacent strikes. The implied volatility is given by:
$$
\sigma_{\text{VIX}}=\sqrt{\sigma^2_{\text{VIX}}}
$$
We can directly load the data:

```{r}
getSymbols("^VIX", src = "yahoo",
           from = "2009-01-01",
           to   = Sys.Date())
vix <- data.frame(date = index(VIX), coredata(VIX)) |>
  drop_na() |>
  arrange(date)
```

Plotting the data:
```{r}
ggplot(vix, aes(x=date, y=VIX.Adjusted)) +
  geom_line() +
  annotate("rect", xmin = as.Date("2017-01-20"),xmax = as.Date("2020-01-20"),
           ymin = 0, ymax = Inf, alpha = 0.2, fill = "red") +
  theme(axis.text.x = element_text(angle = 50, hjust = 1)) +
  annotate("text", x = as.Date("2018-07-01"), y = 60,
           label = "Trump 45", color = "red") +
  annotate("rect", xmin = as.Date("2025-01-20"),xmax = as.Date(Sys.Date()),
           ymin = 0, ymax = Inf, alpha = 0.2, fill = "red") +
  annotate("text", x = as.Date("2025-04-01"), y = 60,
           label = "Trump 47", color = "red") +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y")
```
Once again, we observe similar shapes as for realized volatility. This is mainly explained by the global stability of the S&P500 index, which allows smoother comparisons. \hfill \break
Notably, two spikes appear: the upward spike right after DJT's presidency, probably due to COVID-19 and right after its inauguration, beginning of 2025.

## Analyzing relationships between the markets and the orange man's bamboozles

### Using a simple regression of IV and RV on the Tweets and Truths



### Using a GARCH model to observe how DJT's media actions enter volatility dynamics


